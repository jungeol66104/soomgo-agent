#!/usr/bin/env python3
"""
2-Stage Hybrid Pipeline for Finding Natural Conversations
Stage 1: Heuristic Filter (fast, free)
Stage 2: LLM Judge (accurate, affordable)

Key Feature: Filters out system messages and templates to analyze ONLY human conversation
"""

import json
import os
from pathlib import Path
from typing import List, Dict, Tuple
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# Initialize OpenAI client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


# ============================================================================
# MESSAGE CLEANING: Remove Templates and System Messages
# ============================================================================

# Template patterns that indicate auto-generated service descriptions
TEMPLATE_PATTERNS = [
    "ì§€ë‚œ ì‹œì¦Œ í•©ê²©ë¥  80%",
    "[ì„œë¹„ìŠ¤ ì•ˆë‚´]",
    "ë¼ì´íŠ¸ íŒ¨í‚¤ì§€",
    "ë§ì¶¤í˜• ìì†Œì„œ/ë©´ì ‘ íŒ¨í‚¤ì§€",
    "[í•©ê²© ì‚¬ë¡€ ë° ê°•ì ]",
    "ì‚¼ì„±ì—”ì§€ë‹ˆì–´ë§, LGì „ì, YGì—”í„°í…Œì¸ë¨¼íŠ¸",
    "ë¨¼ì € 1ë§Œì› ë§›ë³´ê¸° ì²¨ì‚­ìœ¼ë¡œ",
    "ë©´ì ‘ ì›ìƒ· ë§ˆìŠ¤í„° ì½”ì¹­",
    "í¬íŠ¸í´ë¦¬ì˜¤ ì œì‘ ì„œë¹„ìŠ¤ ì•ˆë‚´",
    "[í•µì‹¬ ë©´ì ‘ Q-Pack]",
    "í¬íŠ¸í´ë¦¬ì˜¤ ê¸°íš (ìŠ¤í† ë¦¬ ì„¤ê³„)",
]

# System message types to always filter out
SYSTEM_MESSAGE_TYPES = {
    'ST_QUOTE',     # Welcome message
    'ST_002',       # Cache refund notification
    'ST_003',       # Quote viewed notification
    'SP_ST_002',    # Soomgo Pay recommendation
    'SP_SB_001',    # System notifications
    'SB_005',       # System notifications
    'SB_001',       # System notifications
    'RQ_001',       # Request context (not conversation)
}


def is_template_message(message: str) -> bool:
    """Check if message is a system-generated template."""
    if not message or len(message) < 50:
        return False

    # Long messages with template keywords are likely templates
    if len(message) > 400:
        for pattern in TEMPLATE_PATTERNS:
            if pattern in message:
                return True

    return False


def is_file_upload_message(message: str) -> bool:
    """Check if message is just a file upload notification."""
    return message.strip() == "ì‚¬ì§„ì„ ë³´ëƒˆìŠµë‹ˆë‹¤." or message.strip() == "íŒŒì¼ì„ ë³´ëƒˆìŠµë‹ˆë‹¤."


def clean_messages(messages: List[Dict]) -> List[Dict]:
    """
    Filter out system messages, templates, and file uploads.
    Returns only real human conversation messages.
    """
    cleaned = []

    for msg in messages:
        # Skip if no message content
        if not msg.get('message'):
            continue

        # Skip system message types
        if msg.get('type') in SYSTEM_MESSAGE_TYPES:
            continue

        # Skip system user (id=0)
        if msg['user']['id'] == 0:
            continue

        # Skip template messages
        if is_template_message(msg['message']):
            continue

        # Skip file upload notifications
        if is_file_upload_message(msg['message']):
            continue

        cleaned.append(msg)

    return cleaned


# ============================================================================
# STAGE 1: HEURISTIC FILTER
# ============================================================================

def load_conversation(chat_file: Path) -> List[Dict]:
    """Load messages from a chat file."""
    messages = []
    with open(chat_file, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                msg = json.loads(line)
                messages.append(msg)
            except json.JSONDecodeError:
                continue
    return messages


def heuristic_filter(messages: List[Dict]) -> bool:
    """
    Stage 1: Fast heuristic filter on CLEANED messages.
    Returns True if conversation is a candidate for Stage 2.
    """
    # First, clean the messages (remove templates and system messages)
    clean = clean_messages(messages)

    # Must have at least 3 real human messages
    if len(clean) < 3:
        return False

    # Must have at least 2 different people
    user_ids = [m['user']['id'] for m in clean]
    if len(set(user_ids)) < 2:
        return False

    # Check for back-and-forth (not all from same user)
    first_user = user_ids[0]
    if user_ids.count(first_user) > len(user_ids) * 0.8:  # One user dominates >80%
        return False

    # Total conversation length after cleaning
    total_chars = sum(len(m['message']) for m in clean)
    if total_chars < 50 or total_chars > 2000:
        return False

    # At least one message should be substantial (10-300 chars)
    substantial = [m for m in clean if 10 <= len(m['message']) <= 300]
    if len(substantial) < 2:
        return False

    return True


def run_stage1(data_dir: Path, max_candidates: int = 500) -> List[Tuple[Path, List[Dict]]]:
    """
    Stage 1: Run heuristic filter on all conversations.
    Returns list of (file_path, messages) tuples.
    """
    messages_dir = data_dir / 'messages'
    chat_files = list(messages_dir.glob('chat_*.jsonl'))

    print(f"ğŸ” Stage 1: Heuristic Filter")
    print(f"   Total conversations: {len(chat_files)}")

    candidates = []

    for i, chat_file in enumerate(chat_files):
        if i % 500 == 0:
            print(f"   Processed: {i}/{len(chat_files)}...")

        messages = load_conversation(chat_file)

        if heuristic_filter(messages):
            candidates.append((chat_file, messages))

            if len(candidates) >= max_candidates:
                break

    print(f"   âœ“ Filtered to {len(candidates)} candidates")
    return candidates


# ============================================================================
# STAGE 2: LLM JUDGE
# ============================================================================

def format_conversation_for_llm(messages: List[Dict]) -> str:
    """
    Format conversation for LLM evaluation.
    ONLY includes cleaned human messages (no templates, no system messages).
    """
    # Clean messages first
    clean = clean_messages(messages)

    # Track users
    user_map = {}
    lines = []

    for msg in clean:
        user_id = msg['user']['id']
        if user_id not in user_map:
            user_map[user_id] = f"Person{len(user_map)+1}"

        user_label = user_map[user_id]
        text = msg['message']
        lines.append(f"{user_label}: {text}")

    return '\n'.join(lines)


def llm_score_conversation(conversation_text: str) -> Tuple[float, str]:
    """
    Stage 2: Use LLM to score conversation naturalness.
    Returns (score, reasoning).
    """

    prompt = f"""ë‹¹ì‹ ì€ ëŒ€í™”ì˜ ìì—°ìŠ¤ëŸ¬ì›€ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ëŒ€í™”ë¥¼ ì½ê³  **1~10ì ** ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”.

**í‰ê°€ ê¸°ì¤€:**
- ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í‘œí˜„ (formal vs casual ì ì ˆì„±)
- ë§¥ë½ê³¼ íë¦„ì´ ìì—°ìŠ¤ëŸ¬ìš´ê°€
- ì‹¤ì œ ì‚¬ëŒê°™ì€ í†¤ì¸ê°€, ì•„ë‹ˆë©´ ë¡œë´‡/í…œí”Œë¦¿ ê°™ì€ê°€
- ê°ì •ê³¼ ê³µê°ì´ ëŠê»´ì§€ëŠ”ê°€
- ê³¼ë„í•œ ì¡´ëŒ“ë§, ì´ëª¨í‹°ì½˜, ë°˜ë³µì  íŒ¨í„´ì´ ìˆëŠ”ê°€

**ì ìˆ˜ ê¸°ì¤€:**
- 1-3ì : ë§¤ìš° ë¶€ìì—°ìŠ¤ëŸ¬ì›€ (ë¡œë´‡ ê°™ìŒ, í…œí”Œë¦¿)
- 4-6ì : ë³´í†µ (ì•½ê°„ ì–´ìƒ‰í•˜ì§€ë§Œ ëŒ€í™”ëŠ” ë¨)
- 7-9ì : ìì—°ìŠ¤ëŸ¬ì›€ (ì‹¤ì œ ì‚¬ëŒ ê°™ìŒ)
- 10ì : ì™„ë²½í•¨ (ë§¤ìš° ìì—°ìŠ¤ëŸ½ê³  ì¸ê°„ì )

**ëŒ€í™”:**
{conversation_text}

**ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ JSONìœ¼ë¡œ):**
{{
  "score": <1~10 ì‚¬ì´ ìˆ«ì>,
  "reasoning": "<í•œ ì¤„ í‰ê°€ ì´ìœ >"
}}
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an expert at evaluating conversation naturalness. Always respond in valid JSON format."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        result = json.loads(response.choices[0].message.content)
        return result['score'], result['reasoning']

    except Exception as e:
        print(f"   âš ï¸  LLM error: {e}")
        return 0.0, "Error in scoring"


def run_stage2(candidates: List[Tuple[Path, List[Dict]]], top_n: int = 10) -> List[Dict]:
    """
    Stage 2: Score candidates with LLM and return top N.
    """
    print(f"\nğŸ¤– Stage 2: LLM Judge")
    print(f"   Scoring {len(candidates)} candidates...")

    results = []

    for i, (chat_file, messages) in enumerate(candidates):
        if i % 50 == 0:
            print(f"   Scored: {i}/{len(candidates)}...")

        # Get cleaned conversation text for LLM
        conversation_text = format_conversation_for_llm(messages)

        # Skip if cleaned conversation is too short
        if len(conversation_text) < 20:
            continue

        score, reasoning = llm_score_conversation(conversation_text)

        # Count messages before and after cleaning
        clean_count = len(clean_messages(messages))
        raw_count = len([m for m in messages if m.get('message')])

        results.append({
            'file': chat_file.name,
            'score': score,
            'reasoning': reasoning,
            'conversation': conversation_text,
            'raw_messages': messages,
            'clean_count': clean_count,
            'raw_count': raw_count,
        })

    # Sort by score descending
    results.sort(key=lambda x: x['score'], reverse=True)

    if results:
        print(f"   âœ“ Top score: {results[0]['score']}")
        print(f"   âœ“ Median score: {results[len(results)//2]['score']}")

    return results[:top_n]


# ============================================================================
# MAIN
# ============================================================================

def main():
    """Run the full 2-stage pipeline."""

    print("="*80)
    print("FINDING MOST NATURAL CONVERSATIONS")
    print("="*80)

    # Find latest export
    export_dir = Path('export')
    exports = sorted(export_dir.glob('*_export'))

    if not exports:
        print("âŒ No exports found!")
        return

    latest_export = exports[-1]
    print(f"ğŸ“ Using export: {latest_export.name}\n")

    data_dir = latest_export / 'data'

    # Stage 1: Heuristic filter
    candidates = run_stage1(data_dir, max_candidates=500)

    if not candidates:
        print("âŒ No candidates found after Stage 1")
        return

    # Stage 2: LLM scoring
    top_10 = run_stage2(candidates, top_n=10)

    # Display results
    print("\n" + "="*80)
    print("TOP 10 MOST NATURAL CONVERSATIONS")
    print("="*80)

    for i, result in enumerate(top_10, 1):
        print(f"\n[{i}] Score: {result['score']}/10 | {result['file']}")
        print(f"Messages: {result['raw_count']} raw â†’ {result['clean_count']} cleaned")
        print(f"Reasoning: {result['reasoning']}")
        print("-" * 80)
        print(result['conversation'])
        print()

    # Save results to data/analysis/
    output_dir = Path('data/analysis')
    output_dir.mkdir(parents=True, exist_ok=True)

    output_json = output_dir / 'best_conversations.json'
    output_txt = output_dir / 'best_conversations.txt'

    # Save JSON (for programmatic use)
    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(top_10, f, ensure_ascii=False, indent=2)

    # Save TXT (for human reading)
    with open(output_txt, 'w', encoding='utf-8') as f:
        f.write("TOP 10 MOST NATURAL CONVERSATIONS\n")
        f.write("(Templates and system messages filtered out)\n")
        f.write("="*80 + "\n\n")

        for i, result in enumerate(top_10, 1):
            f.write(f"[{i}] Score: {result['score']}/10 | {result['file']}\n")
            f.write(f"Messages: {result['raw_count']} raw â†’ {result['clean_count']} cleaned\n")
            f.write(f"Reasoning: {result['reasoning']}\n")
            f.write("-"*80 + "\n")
            f.write(result['conversation'] + "\n")
            f.write("\n\n")

    print(f"âœ… Results saved:")
    print(f"   - {output_json} (JSON)")
    print(f"   - {output_txt} (Text)")


if __name__ == '__main__':
    main()
