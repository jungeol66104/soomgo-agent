"""Core agent implementation."""

import json
import operator
import os
from pathlib import Path
from typing import Annotated, Literal, Optional, TypedDict

from dotenv import load_dotenv
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from loguru import logger

from .config import AgentConfig
from src.knowledge import KnowledgeRetriever

# Load environment
load_dotenv()


@tool
def count_characters(text: str, include_spaces: bool = True) -> dict:
    """
    Count characters in a text string accurately.

    Use this tool when you need to count characters precisely. DO NOT guess or estimate.

    Args:
        text: The text to count characters in
        include_spaces: If True, count spaces. If False, exclude spaces. Default is True.

    Returns:
        Dictionary with character counts:
        - total_with_spaces: Total character count including spaces
        - total_without_spaces: Total character count excluding spaces
        - spaces: Number of space characters
        - lines: Number of lines
    """
    total_with_spaces = len(text)
    total_without_spaces = len(text.replace(" ", "").replace("\n", "").replace("\t", ""))
    spaces = text.count(" ")
    lines = text.count("\n") + 1 if text else 0

    return {
        "total_with_spaces": total_with_spaces,
        "total_without_spaces": total_without_spaces,
        "spaces": spaces,
        "lines": lines,
        "result": total_with_spaces if include_spaces else total_without_spaces
    }


class GatheredInfo(TypedDict):
    """Information to gather from customer."""
    service_type: Optional[str]      # ì„œë¹„ìŠ¤ ì¢…ë¥˜
    company_role: Optional[str]      # íšŒì‚¬/ì§ë¬´
    deadline: Optional[str]          # ë§ˆê°ì¼
    experience: Optional[str]        # ê²½ë ¥
    existing_resume: Optional[str]   # ê¸°ì¡´ ìì†Œì„œ (optional)
    difficulties: Optional[str]      # ì–´ë ¤ì›€
    budget: Optional[str]            # ì˜ˆì‚°


class ChatState(TypedDict):
    """State for the conversation."""
    messages: Annotated[list, operator.add]
    gathered_info: GatheredInfo
    conversation_state: Literal["active", "waiting", "deferred", "closed"]
    last_closure_response: Optional[str]
    retrieved_knowledge: Optional[str]  # Knowledge from retrieval system


class SoomgoAgent:
    """Soomgo provider agent."""

    def __init__(self, config: Optional[AgentConfig] = None):
        """
        Initialize agent.

        Args:
            config: Agent configuration (defaults to AgentConfig.from_env())
        """
        self.config = config or AgentConfig.from_env()
        self.system_prompt = self._load_prompt()

        # Initialize knowledge retriever
        logger.info("Initializing knowledge retriever...")
        self.retriever = KnowledgeRetriever(data_dir=str(self.config.knowledge_dir))

        self.graph = self._build_graph()

        logger.info(f"Initialized SoomgoAgent with {self.config.model}")

    def _load_prompt(self) -> str:
        """Load system prompt from file."""
        prompt_path = self.config.prompt_path

        if not prompt_path.exists():
            logger.warning(f"Prompt file not found: {prompt_path}")
            return "ë‹¹ì‹ ì€ ìˆ¨ê³  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê³ ê°ì—ê²Œ ì¹œì ˆí•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”."

        with open(prompt_path, 'r', encoding='utf-8') as f:
            prompt = f.read().strip()

        logger.info(f"Loaded prompt from {prompt_path} ({len(prompt)} chars)")
        return prompt

    def _build_graph(self) -> CompiledStateGraph:
        """Build LangGraph workflow with information extraction and knowledge retrieval."""
        graph_builder = StateGraph(ChatState)

        # Add nodes
        graph_builder.add_node("extract_info", self._extract_information)
        graph_builder.add_node("retrieve_knowledge", self._retrieve_knowledge)
        graph_builder.add_node("agent", self._run_agent)

        # Build workflow: START -> extract info -> retrieve knowledge -> generate response -> END
        graph_builder.add_edge(START, "extract_info")
        graph_builder.add_edge("extract_info", "retrieve_knowledge")
        graph_builder.add_edge("retrieve_knowledge", "agent")
        graph_builder.add_edge("agent", END)

        return graph_builder.compile()

    def _extract_information(self, state: ChatState) -> dict:
        """Extract information and conversation state from user's latest message."""
        messages = state["messages"]
        current_info = state.get("gathered_info", {})
        current_conv_state = state.get("conversation_state", "active")
        last_closure = state.get("last_closure_response")

        # Get latest user message
        user_messages = [m for m in messages if isinstance(m, HumanMessage)]
        if not user_messages:
            return {
                "gathered_info": current_info,
                "conversation_state": current_conv_state,
                "last_closure_response": last_closure
            }

        latest_message = user_messages[-1].content

        # Get last agent message to check if we already gave closure response
        agent_messages = [m for m in messages if isinstance(m, AIMessage)]
        last_agent_msg = agent_messages[-1].content if agent_messages else None

        # Extraction prompt with conversation state detection
        extraction_prompt = f"""ë‹¹ì‹ ì€ ê³ ê° ë©”ì‹œì§€ì—ì„œ í•„ìš”í•œ ì •ë³´ì™€ ëŒ€í™” ìƒíƒœë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ê³ ê° ë©”ì‹œì§€:**
{latest_message}

**í˜„ì¬ ëŒ€í™” ìƒíƒœ:** {current_conv_state}
**ì´ì „ ì—ì´ì „íŠ¸ ì‘ë‹µ:** {last_agent_msg if last_agent_msg else "ì—†ìŒ"}

**ì‘ì—… 1: ì •ë³´ ì¶”ì¶œ**
ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš” (ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ null):
- service_type: ì„œë¹„ìŠ¤ ì¢…ë¥˜ (ìì†Œì„œ, ì´ë ¥ì„œ, ë©´ì ‘, í¬íŠ¸í´ë¦¬ì˜¤ ë“±)
- company_role: íšŒì‚¬/ì§ë¬´
- deadline: ë§ˆê°ì¼
- experience: ê²½ë ¥
- existing_resume: ê¸°ì¡´ ìì†Œì„œ ë³´ìœ  ì—¬ë¶€
- difficulties: ì–´ë ¤ì›€/ê³ ë¯¼
- budget: ì˜ˆì‚°

**í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ì •ë³´:**
{json.dumps(current_info, ensure_ascii=False)}

**ì‘ì—… 2: ëŒ€í™” ìƒíƒœ ê°ì§€**
ê³ ê°ì˜ ë©”ì‹œì§€ë¥¼ ë³´ê³  ëŒ€í™” ìƒíƒœë¥¼ íŒë‹¨í•˜ì„¸ìš”:

- "active": ì •ë³´ ìˆ˜ì§‘ ì¤‘, ì§ˆë¬¸ì— ë‹µë³€ ì¤‘
- "waiting": íŒŒì¼/ìë£Œë¥¼ ë³´ë‚¸ë‹¤ê³  í•¨ (ì˜ˆ: "ë³´ë‚¼ê²Œìš”", "íŒŒì¼ ë³´ë‚´ë“œë¦´ê²Œìš”")
- "deferred": ê³ ë¯¼/ë³´ë¥˜í•œë‹¤ê³  í•¨ (ì˜ˆ: "ê³ ë ¤í•´ë³¼ê²Œìš”", "ìƒê°í•´ë³¼ê²Œìš”", "ë‹¤ì‹œ ì—°ë½ë“œë¦´ê²Œìš”")
- "closed": deferred ìƒíƒœ í›„ ì§§ì€ í™•ì¸ë§Œ í•¨ (ì˜ˆ: "ë„¤", "ë„¤!", "ê°ì‚¬í•©ë‹ˆë‹¤")

**íŒë‹¨ ê·œì¹™:**
1. ê³ ê°ì´ "ê³ ë ¤í•´ë³¼ê²Œìš”", "ìƒê°í•´ë³¼ê²Œìš”", "ë‹¤ì‹œ ì—°ë½ë“œë¦´ê²Œìš”" â†’ deferred
2. í˜„ì¬ ìƒíƒœê°€ deferredì´ê³ , ê³ ê°ì´ "ë„¤", "ë„¤!", "ì•Œê² ìŠµë‹ˆë‹¤", "ê°ì‚¬í•©ë‹ˆë‹¤"ë§Œ ë³´ëƒ„ â†’ closed
3. ê³ ê°ì´ "íŒŒì¼ ë³´ë‚¼ê²Œìš”", "ìì†Œì„œ ë³´ë‚´ë“œë¦´ê²Œìš”" â†’ waiting
4. ê·¸ ì™¸ â†’ active

**ì¶œë ¥ í˜•ì‹ (JSON):**
{{
  "service_type": "...",
  "company_role": "...",
  "deadline": "...",
  "experience": "...",
  "existing_resume": "...",
  "difficulties": "...",
  "budget": "...",
  "conversation_state": "active|waiting|deferred|closed"
}}
"""

        # Call LLM with JSON mode
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            extractor = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0.0,
                api_key=api_key,
            )

            response = extractor.invoke(
                [HumanMessage(content=extraction_prompt)],
                response_format={"type": "json_object"}
            )

            extracted = json.loads(response.content)

            # Extract conversation state
            new_conv_state = extracted.pop("conversation_state", current_conv_state)

            # Merge info with current info (only update non-null values)
            updated_info = current_info.copy()
            for key, value in extracted.items():
                if value is not None and value != "null" and value.strip() != "":
                    updated_info[key] = value

            logger.debug(f"Extracted info: {extracted}")
            logger.debug(f"Conversation state: {current_conv_state} -> {new_conv_state}")
            logger.debug(f"Updated info: {updated_info}")

            return {
                "gathered_info": updated_info,
                "conversation_state": new_conv_state,
                "last_closure_response": last_closure
            }

        except Exception as e:
            logger.error(f"Error extracting information: {e}")
            return {
                "gathered_info": current_info,
                "conversation_state": current_conv_state,
                "last_closure_response": last_closure
            }

    def _retrieve_knowledge(self, state: ChatState) -> dict:
        """Retrieve relevant knowledge for the user's latest message."""
        messages = state["messages"]
        gathered_info = state.get("gathered_info", {})

        # Get latest user message
        user_messages = [m for m in messages if isinstance(m, HumanMessage)]
        if not user_messages:
            return {"retrieved_knowledge": None}

        latest_message = user_messages[-1].content

        # Build context-aware query
        # If user asks generic question like "ì–¼ë§ˆì¸ê°€ìš”?", add service context
        query = latest_message
        service_type = gathered_info.get("service_type")

        # Enhance query with context if it's too generic
        if len(latest_message) < 20 and service_type:
            # Generic queries like "ì–¼ë§ˆì¸ê°€ìš”?", "ê°€ê²©ì€?"
            query = f"{service_type} {latest_message}"
            logger.debug(f"Enhanced query with context: '{latest_message}' -> '{query}'")

        try:
            # Retrieve knowledge (lower threshold for better recall)
            retrieved = self.retriever.retrieve(query, top_k=3, threshold=0.4)

            # Format knowledge
            if retrieved.get("structured") or retrieved.get("faqs"):
                formatted = self.retriever.format_knowledge(retrieved)
                logger.debug(f"Retrieved knowledge ({len(formatted)} chars)")
                return {"retrieved_knowledge": formatted}
            else:
                logger.debug("No relevant knowledge found")
                return {"retrieved_knowledge": None}

        except Exception as e:
            logger.error(f"Error retrieving knowledge: {e}")
            return {"retrieved_knowledge": None}

    def _run_agent(self, state: ChatState) -> dict:
        """Run agent node with state-aware prompting."""
        messages = state["messages"]
        gathered_info = state.get("gathered_info", {})
        conv_state = state.get("conversation_state", "active")
        last_closure = state.get("last_closure_response")
        retrieved_knowledge = state.get("retrieved_knowledge")

        # Build state-aware system prompt with conversation state
        state_summary = self._build_state_summary(gathered_info)
        conv_state_instructions = self._build_conversation_state_instructions(conv_state, last_closure)

        # Build knowledge section if available
        knowledge_section = ""
        if retrieved_knowledge:
            knowledge_section = f"""

## ğŸ“š ê´€ë ¨ ì§€ì‹ (ì°¸ê³ ìš©)
{retrieved_knowledge}

**ì§€ì‹œì‚¬í•­:** ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ë˜, ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”. ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ ì½ì§€ ë§ê³  ëŒ€í™” ë§¥ë½ì— ë§ê²Œ ì‚¬ìš©í•˜ì„¸ìš”."""

        full_prompt = f"""{self.system_prompt}

{state_summary}

{conv_state_instructions}{knowledge_section}"""

        # Add system message if needed
        if not any(isinstance(m, SystemMessage) for m in messages):
            messages = [SystemMessage(content=full_prompt)] + messages
        else:
            # Update existing system message with state
            messages = [SystemMessage(content=full_prompt)] + [m for m in messages if not isinstance(m, SystemMessage)]

        # Get API key
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in .env")

        # Initialize model
        model = ChatOpenAI(
            model=self.config.model,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            api_key=api_key,
        )

        # Bind tools to model
        tools = [count_characters]
        model_with_tools = model.bind_tools(tools)

        # Generate response with tool support
        try:
            response = model_with_tools.invoke(messages)

            # Handle tool calls if any
            while response.tool_calls:
                logger.debug(f"Tool calls detected: {len(response.tool_calls)}")

                # Add AI message with tool calls to history
                messages.append(response)

                # Execute each tool call
                for tool_call in response.tool_calls:
                    tool_name = tool_call["name"]
                    tool_args = tool_call["args"]
                    tool_id = tool_call["id"]

                    logger.debug(f"Executing tool: {tool_name} with args: {tool_args}")

                    # Execute the tool
                    if tool_name == "count_characters":
                        tool_result = count_characters.invoke(tool_args)
                    else:
                        tool_result = {"error": f"Unknown tool: {tool_name}"}

                    # Create tool message with result
                    tool_message = ToolMessage(
                        content=str(tool_result),
                        tool_call_id=tool_id
                    )
                    messages.append(tool_message)

                # Get next response from model
                response = model_with_tools.invoke(messages)

            response_text = response.content
            logger.debug(f"Generated response: {len(response_text)} chars")

            # Track closure responses to prevent repetition
            updated_closure = last_closure
            if conv_state in ["deferred", "waiting"]:
                # Check if response contains closure phrases
                closure_phrases = ["í¸í•˜ì‹¤ ë•Œ", "ê¸°ë‹¤ë¦´ê²Œìš”", "ì—°ë½ ì£¼ì„¸ìš”", "ì–¸ì œë“ ì§€"]
                if any(phrase in response_text for phrase in closure_phrases):
                    updated_closure = response_text
                    logger.debug(f"Tracked closure response: {updated_closure[:50]}...")

            return {
                "messages": [response],
                "last_closure_response": updated_closure
            }

        except Exception as e:
            logger.error(f"Error generating response: {e}")
            error_msg = AIMessage(
                content="ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )
            return {"messages": [error_msg]}

    def _build_state_summary(self, gathered_info: dict) -> str:
        """Build state summary for system prompt."""
        # Required fields
        required_fields = {
            "service_type": "ì„œë¹„ìŠ¤ ì¢…ë¥˜",
            "company_role": "íšŒì‚¬/ì§ë¬´",
            "deadline": "ë§ˆê°ì¼",
            "experience": "ê²½ë ¥",
            "difficulties": "ì–´ë ¤ì›€",
            "budget": "ì˜ˆì‚°"
        }

        # Optional field
        optional_fields = {
            "existing_resume": "ê¸°ì¡´ ìì†Œì„œ"
        }

        # Check what we have
        collected = []
        missing = []

        for key, label in required_fields.items():
            value = gathered_info.get(key)
            if value:
                collected.append(f"- {label}: {value}")
            else:
                missing.append(f"- {label} âœ—")

        # Add optional field if present
        for key, label in optional_fields.items():
            value = gathered_info.get(key)
            if value:
                collected.append(f"- {label} (ì„ íƒ): {value}")

        # Build summary
        if not collected and not missing:
            return """
## í˜„ì¬ ìƒíƒœ

ì•„ì§ ìˆ˜ì§‘ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ í†µí•´ ë‹¤ìŒ ì •ë³´ë¥¼ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤:
- ì„œë¹„ìŠ¤ ì¢…ë¥˜ (ìì†Œì„œ, ì´ë ¥ì„œ, ë©´ì ‘, í¬íŠ¸í´ë¦¬ì˜¤ ë“±)
- íšŒì‚¬/ì§ë¬´
- ë§ˆê°ì¼
- ê²½ë ¥ (ì‹ ì…/ê²½ë ¥)
- ì–´ë ¤ì›€ì´ë‚˜ ê³ ë¯¼
- ì˜ˆì‚°

**ì¤‘ìš”**: í•œ ë²ˆì— ëª¨ë“  ê²ƒì„ ë¬¼ì–´ë³´ì§€ ë§ˆì„¸ìš”. ê³ ê°ì´ ë°©ê¸ˆ ë§í•œ ë‚´ìš©ê³¼ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ëŠ” 1-2ê°€ì§€ë§Œ ë¬¼ì–´ë³´ì„¸ìš”.
"""

        summary = "\n## í˜„ì¬ ì§„í–‰ ìƒí™©\n"

        if collected:
            summary += "\n### âœ“ íŒŒì•…ëœ ì •ë³´:\n" + "\n".join(collected)

        if missing:
            summary += "\n\n### ì•„ì§ í•„ìš”í•œ ì •ë³´:\n" + "\n".join(missing)

        summary += """

**ì¤‘ìš”**: ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„ì„ ìœ ì§€í•˜ì„¸ìš”. ê³ ê°ì´ ë°©ê¸ˆ ë§í•œ ë‚´ìš©ê³¼ ì—°ê²°ë˜ëŠ” 1-2ê°€ì§€ë§Œ ë¬¼ì–´ë³´ì„¸ìš”. í•„ìš”í•œ ëª¨ë“  ì •ë³´ë¥¼ í•œ ë²ˆì— ë¬¼ì–´ë³´ì§€ ë§ˆì„¸ìš”.
"""

        return summary

    def _build_conversation_state_instructions(self, conv_state: str, last_closure: Optional[str]) -> str:
        """Build dynamic instructions based on conversation state."""

        if conv_state == "closed":
            return """
## ğŸš¨ ëŒ€í™” ì¢…ë£Œ ìƒíƒœ

ê³ ê°ì´ ì´ë¯¸ ê³ ë¯¼/ë³´ë¥˜ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ëŒ€í™”ëŠ” ëë‚¬ìŠµë‹ˆë‹¤.

**ì§€ì‹œì‚¬í•­:**
- "ë„¤!" í•˜ë‚˜ë§Œ ì‘ë‹µí•˜ì„¸ìš”
- ë˜ëŠ” ì•„ë¬´ ì‘ë‹µë„ í•˜ì§€ ë§ˆì„¸ìš”
- ì ˆëŒ€ "ê¸°ë‹¤ë¦´ê²Œìš”", "í¸í•˜ì‹¤ ë•Œ", "ì–¸ì œë“ ì§€" ê°™ì€ ë§ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
- ì´ë¯¸ ë§ˆë¬´ë¦¬ ì¸ì‚¬ë¥¼ í–ˆìŠµë‹ˆë‹¤

**ì˜ˆì‹œ:**
ê³ ê°: "ë„¤!"
ë‚˜: "ë„¤!"
"""

        elif conv_state == "deferred":
            if last_closure:
                return f"""
## âš ï¸ ëŒ€í™” ë³´ë¥˜ ì¤‘ - ì´ë¯¸ ë§ˆë¬´ë¦¬ ì¸ì‚¬í•¨

ê³ ê°ì´ ê³ ë¯¼í•˜ê² ë‹¤ê³  í–ˆê³ , ë‹¹ì‹ ì€ ì´ë¯¸ ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤:
"{last_closure[:100]}..."

**ì§€ì‹œì‚¬í•­:**
- ê°™ì€ ë‚´ìš© ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
- ê³ ê°ì´ ë‹¤ì‹œ "ë„¤"ë¼ê³ ë§Œ í•˜ë©´ "ë„¤!" í•˜ë‚˜ë¡œ ì‘ë‹µ
- ë” ì´ìƒ "ê¸°ë‹¤ë¦´ê²Œìš”" ê°™ì€ ë§ í•˜ì§€ ë§ˆì„¸ìš”
"""
            else:
                return """
## â¸ï¸ ëŒ€í™” ë³´ë¥˜ ìƒíƒœ

ê³ ê°ì´ ê³ ë¯¼/ë³´ë¥˜í•˜ê² ë‹¤ê³  í–ˆìŠµë‹ˆë‹¤.

**ì§€ì‹œì‚¬í•­:**
- ê°„ë‹¨íˆ í•œ ë²ˆë§Œ ì‘ë‹µí•˜ì„¸ìš”: "í¸í•˜ì‹¤ ë•Œ ì—°ë½ ì£¼ì„¸ìš”" ë˜ëŠ” "ë„¤! ê³ ë¯¼í•´ë³´ì‹œê³  í¸í•˜ì‹¤ ë•Œ ì—°ë½ ì£¼ì„¸ìš”"
- 20-30ì ì •ë„ë¡œ ì§§ê²Œ
- ì´ ì‘ë‹µ í›„, ê³ ê°ì´ "ë„¤"ë¼ê³ ë§Œ í•˜ë©´ "ë„¤!"ë¡œë§Œ ë‹µí•˜ì„¸ìš”
"""

        elif conv_state == "waiting":
            if last_closure:
                return f"""
## ğŸ“ íŒŒì¼ ëŒ€ê¸° ì¤‘ - ì´ë¯¸ í™•ì¸í•¨

ê³ ê°ì´ íŒŒì¼ì„ ë³´ë‚¸ë‹¤ê³  í–ˆê³ , ë‹¹ì‹ ì€ ì´ë¯¸ ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤:
"{last_closure[:100]}..."

**ì§€ì‹œì‚¬í•­:**
- ê°™ì€ ë‚´ìš© ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
- ê³ ê°ì´ "ë„¤"ë¼ê³ ë§Œ í•˜ë©´ "ë„¤!" ë˜ëŠ” "ê¸°ë‹¤ë¦´ê²Œìš”!" ì§§ê²Œë§Œ
- ë” ì´ìƒ ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”
"""
            else:
                return """
## ğŸ“ íŒŒì¼ ëŒ€ê¸° ìƒíƒœ

ê³ ê°ì´ íŒŒì¼ì´ë‚˜ ìë£Œë¥¼ ë³´ë‚¸ë‹¤ê³  í–ˆìŠµë‹ˆë‹¤.

**ì§€ì‹œì‚¬í•­:**
- "ê¸°ë‹¤ë¦´ê²Œìš”!" ë˜ëŠ” "íŒŒì¼ í™•ì¸í•˜ê³  ë°”ë¡œ ë„ì™€ë“œë¦´ê²Œìš”" ì§§ê²Œë§Œ
- 10-20ì ì •ë„
- ì´ ì‘ë‹µ í›„, ê³ ê°ì´ "ë„¤"ë¼ê³ ë§Œ í•˜ë©´ "ë„¤!"ë¡œë§Œ ë‹µí•˜ì„¸ìš”
"""

        else:  # active
            return """
## âœ… í™œì„± ëŒ€í™” ìƒíƒœ

ì¼ë°˜ì ì¸ ì •ë³´ ìˆ˜ì§‘ì´ë‚˜ ëŒ€í™”ê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”.
"""

    def chat(
        self,
        user_message: str,
        conversation_history: Optional[list[dict]] = None,
        gathered_info: Optional[dict] = None,
        conversation_state: Optional[str] = None,
        last_closure_response: Optional[str] = None
    ) -> tuple[str, dict, str, Optional[str]]:
        """
        Send a message and get response.

        Args:
            user_message: Customer's message
            conversation_history: Previous messages [{"role": "user"|"assistant", "content": "..."}]
            gathered_info: Previously gathered information
            conversation_state: Current conversation state
            last_closure_response: Last closure response given

        Returns:
            Tuple of (Agent's response, Updated gathered_info, conversation_state, last_closure_response)
        """
        # Build messages
        messages = []

        if conversation_history:
            for msg in conversation_history:
                if msg["role"] == "user":
                    messages.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    messages.append(AIMessage(content=msg["content"]))

        messages.append(HumanMessage(content=user_message))

        # Initialize gathered_info if not provided
        if gathered_info is None:
            gathered_info = {
                "service_type": None,
                "company_role": None,
                "deadline": None,
                "experience": None,
                "existing_resume": None,
                "difficulties": None,
                "budget": None
            }

        # Initialize conversation state if not provided
        if conversation_state is None:
            conversation_state = "active"

        # Invoke graph
        try:
            result = self.graph.invoke({
                "messages": messages,
                "gathered_info": gathered_info,
                "conversation_state": conversation_state,
                "last_closure_response": last_closure_response
            })
            response = result["messages"][-1].content
            updated_info = result.get("gathered_info", gathered_info)
            updated_state = result.get("conversation_state", conversation_state)
            updated_closure = result.get("last_closure_response", last_closure_response)

            return response, updated_info, updated_state, updated_closure

        except Exception as e:
            logger.error(f"Error in chat: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", gathered_info, conversation_state, last_closure_response

    def reset(self):
        """Reset the agent (currently stateless, but kept for future use)."""
        logger.info("Agent reset requested (currently stateless)")
